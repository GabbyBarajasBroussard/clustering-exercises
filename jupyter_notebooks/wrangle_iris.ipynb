{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp \n",
    "import os\n",
    "import sklearn.preprocessing\n",
    "from env import host, user, password\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_iris_dataset(df):\n",
    "    '''This function takes in the data from the data frame and splits it into train, validate, test.'''\n",
    "    train_validate, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.species)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, random_state=123, stratify=train_validate.species)\n",
    "    return train, validate, test\n",
    "\n",
    "def prep_iris_data():\n",
    "    '''This function reads in the iris dataframe, cleans it and splits it into train, validate, test.'''\n",
    "    #Define the df\n",
    "    df= pd.read_csv(\"iris.csv\", index_col=0)\n",
    "    # Drop the species_id and measurement_id columns\n",
    "    df = df.drop(columns=['species_id'])\n",
    "    \n",
    "    # Rename the species_name column to just species\n",
    "    df = df.rename(columns={'species_name': 'species'})\n",
    "    \n",
    "    # encode the species column\n",
    "    df_dummies = pd.get_dummies(df[['species']], drop_first=True)\n",
    "    df = pd.concat([df, df_dummies], axis=1)\n",
    "    \n",
    "    # split the data\n",
    "    train, validate, test = split_iris_dataset(df)\n",
    "    \n",
    "    return train, validate, test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_scaled_columns(train, validate, test, scaler, columns_to_scale):\n",
    "\n",
    "    new_column_names = [c + '_scaled' for c in columns_to_scale]\n",
    "    scaler.fit(train[columns_to_scale])\n",
    "\n",
    "    train_scaled = pd.concat([\n",
    "        train,\n",
    "        pd.DataFrame(scaler.transform(train[columns_to_scale]), columns=new_column_names, index=train.index)], axis=1)\n",
    "    validate_scaled = pd.concat([\n",
    "        validate,\n",
    "        pd.DataFrame(scaler.transform(validate[columns_to_scale]), columns=new_column_names, index=validate.index)], axis=1)\n",
    "    test_scaled = pd.concat([\n",
    "        test,\n",
    "        pd.DataFrame(scaler.transform(test[columns_to_scale]), columns=new_column_names, index=test.index)], axis=1)\n",
    "    \n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "def scale_iris(train, validate, test):\n",
    "    train_scaled, validate_scaled, test_scaled = add_scaled_columns(\n",
    "    train,\n",
    "    validate,\n",
    "    test,\n",
    "    scaler=sklearn.preprocessing.MinMaxScaler(),\n",
    "    columns_to_scale=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],\n",
    "    )\n",
    "    # drop rows not needed for modeling\n",
    "    cols_to_remove = ['species', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    train_scaled = train_scaled.drop(columns=cols_to_remove)\n",
    "    validate_scaled = validate_scaled.drop(columns=cols_to_remove)\n",
    "    test_scaled = test_scaled.drop(columns=cols_to_remove)\n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "\n",
    "def wrangle_iris_data():\n",
    "    \"\"\"\n",
    "    This function takes acquired iris data, the cleaned data, scales it\n",
    "    and splits the data into train, validate, and test datasets\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('iris.csv', index_col=0)\n",
    "    train, test, validate = prepare.prep_iris_data()\n",
    "    #train_and_validate, test = train_test_split(df, test_size=.15, random_state=123)\n",
    "    #train, validate = train_test_split(train_and_validate, test_size=.15, random_state=123)\n",
    "    # return train, test, validate\n",
    "    train_scaled, validate_scaled, test_scaled = scale_iris(train, validate, test)\n",
    "    return train, validate, test, train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "\n",
    "####### NOTE: to call wrangle_iris_data \n",
    "##### train, validate, test, train_scaled, validate_scaled, test_scaled = wrangle_iris_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
